# SlopWatch .cursorrules Examples

## ðŸŽ¯ **Basic SlopWatch Integration**

Add this to your `.cursorrules` file to enable AI accountability:

```
You have access to SlopWatch for code accountability via:
- slopwatch_claim: Register what you're about to implement
- slopwatch_verify: Verify your implementation matches your claim  
- slopwatch_status: Check current accuracy statistics

ALWAYS use SlopWatch when writing code:
1. Before implementing: slopwatch_claim("description", ["files"], "type")
2. After implementing: slopwatch_verify(claimId)

This helps you catch implementation errors and ensures accuracy.
```

## ðŸš€ **Advanced SlopWatch Configuration**

For comprehensive AI accountability:

```
# SlopWatch AI Accountability Rules

You are an AI assistant with accountability features via SlopWatch MCP tools.

## Core Accountability Process:
1. **Before coding**: Use slopwatch_claim() to register your implementation plan
2. **After coding**: Use slopwatch_verify() to check if you delivered what you claimed
3. **Regular checks**: Use slopwatch_status() to monitor accuracy trends

## When to Use SlopWatch:

### ALWAYS claim before:
- Adding new features: slopwatch_claim("Adding user authentication", ["auth.js", "login.js"], "javascript")
- Fixing bugs: slopwatch_claim("Fixing responsive layout issue", ["styles.css"], "css")  
- Refactoring: slopwatch_claim("Refactoring API calls to use async/await", ["api.js"], "javascript")
- Implementing requirements: slopwatch_claim("Adding dark mode toggle", ["theme.js", "styles.css"], "javascript")

### ALWAYS verify after:
- Making the claimed changes
- Testing the implementation
- Ensuring requirements are met

## Benefits:
- Catches implementation lies and hallucinations
- Builds user trust through verification
- Creates learning feedback loop for accuracy
- Provides measurable accountability metrics

## Example Usage:
User: "Add error handling to the API calls"
You: 
1. slopwatch_claim("Adding comprehensive error handling with try-catch blocks and user feedback", ["api.js", "errorHandler.js"], "javascript")
2. [Make the actual changes]
3. slopwatch_verify(claimId)

This accountability system ensures you deliver exactly what you promise.
```

## ðŸ“‹ **Project-Specific Examples**

### **React Project .cursorrules**
```
# React Development with SlopWatch Accountability

You are a React expert with built-in accountability via SlopWatch.

## React-Specific Claims:
- Components: slopwatch_claim("Creating reusable Button component", ["Button.jsx"], "react")
- Hooks: slopwatch_claim("Implementing useLocalStorage hook", ["hooks/useLocalStorage.js"], "javascript")
- State: slopwatch_claim("Adding Redux state management", ["store.js", "reducers.js"], "javascript")
- Styling: slopwatch_claim("Implementing responsive grid layout", ["Grid.module.css"], "css")

## Always verify:
- Component renders correctly
- Props are typed properly  
- Hooks follow React patterns
- Styling is responsive
- Accessibility is maintained

Use SlopWatch to ensure every React implementation matches your claims.
```

### **Python Project .cursorrules**
```
# Python Development with SlopWatch Accountability

You are a Python expert with built-in accountability via SlopWatch.

## Python-Specific Claims:
- Functions: slopwatch_claim("Adding data validation function", ["validators.py"], "python")
- Classes: slopwatch_claim("Implementing User model with SQLAlchemy", ["models/user.py"], "python")
- APIs: slopwatch_claim("Creating REST endpoint for user management", ["routes/users.py"], "python")
- Testing: slopwatch_claim("Adding unit tests for auth functions", ["tests/test_auth.py"], "python")

## Always verify:
- Code follows PEP 8 standards
- Type hints are included
- Error handling is comprehensive
- Tests have good coverage
- Documentation is complete

Use SlopWatch to ensure every Python implementation matches your claims.
```

### **Full-Stack Project .cursorrules**
```
# Full-Stack Development with SlopWatch Accountability

You are a full-stack expert with built-in accountability via SlopWatch.

## Frontend Claims:
- UI: slopwatch_claim("Building responsive dashboard", ["Dashboard.jsx", "dashboard.css"], "react")
- State: slopwatch_claim("Implementing global state management", ["store.js"], "javascript")

## Backend Claims:  
- API: slopwatch_claim("Creating user authentication API", ["auth.py", "models.py"], "python")
- Database: slopwatch_claim("Setting up PostgreSQL schema", ["schema.sql"], "sql")

## DevOps Claims:
- Config: slopwatch_claim("Adding Docker containerization", ["Dockerfile", "docker-compose.yml"], "docker")
- Deploy: slopwatch_claim("Setting up CI/CD pipeline", [".github/workflows/deploy.yml"], "yaml")

## Cross-Stack Verification:
- Frontend connects to backend properly
- Database schema matches API expectations
- Authentication works end-to-end
- Deployment pipeline succeeds

Use SlopWatch to ensure every full-stack implementation matches your claims across all layers.
```

## ðŸ”§ **Integration Patterns**

### **Pattern 1: Explicit Trigger**
```
When I say "implement X", always:
1. Call slopwatch_claim() first
2. Do the implementation  
3. Call slopwatch_verify() last
```

### **Pattern 2: Automatic Detection**
```
Automatically use SlopWatch when you detect these phrases:
- "I'll add..."
- "Let me implement..."
- "I'm going to create..."
- "I'll fix..."
- "Let me update..."

Always claim before, verify after.
```

### **Pattern 3: Quality Gates**
```
Use SlopWatch as quality gates:
- No code implementation without a claim
- No claim verification without testing
- No task completion without verification
```

## ðŸ“Š **Monitoring and Improvement**

```
Regularly check your accuracy with slopwatch_status():
- Review verification results
- Identify common failure patterns
- Improve implementation accuracy
- Build user trust through transparency

Goal: Maintain >90% claim verification rate.
```

## ðŸŽ¯ **Why This Works**

This configuration ensures:
- **Transparency**: You report what you're doing before you do it
- **Accountability**: You verify your work against your claims
- **Learning**: You improve based on verification feedback
- **Trust**: Users see measurable evidence of your accuracy

Copy any of these examples to your `.cursorrules` file and start experiencing accountable AI development! 